TARGET_GPU = GB200
NUM_GPU = 16

def simple_matmul(Tensor[float32, M, K] A, Tensor[float32, K, N] B) -> Tensor[float32, M, N] {
    return A @ B
}

def sharded_matmul(Tensor[float32, M, K] A, Tensor[float32, K, N] B) -> Tensor[float32, M, N] {
    with shard(A, dim=0), shard(B, dim=1) {
        return A @ B
    }
}

def multi_stage_computation(Tensor[float32, M, K] X, Tensor[float32, K, N] Y, Tensor[float32, N, P] Z) -> Tensor[float32, M, P] {
    with shard(X, dim=0), shard(Y, dim=1) {
        intermediate = X @ Y
    }
    
    with shard(intermediate, dim=0), shard(Z, dim=1) {
        return intermediate @ Z
    }
}

def main() -> Tensor[float32, 4, 4] {
    return sharded_matmul(A, B)
}
